Namespace(K=1, Ks='[20, 40, 60]', alpha=2.0, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=4, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.6931474804878235 BPR_loss 0.6931473016738892 reg_loss 1.7222748738277005e-07
epoch: 0 s: 256000 batch_loss 0.6931279897689819 BPR_loss 0.693116307258606 reg_loss 1.1673946573864669e-05
epoch: 0 s: 512000 batch_loss 0.6925541162490845 BPR_loss 0.6922143697738647 reg_loss 0.0003397430991753936
epoch: 0 s: 768000 batch_loss 0.6901933550834656 BPR_loss 0.6883013248443604 reg_loss 0.0018920134752988815
epoch: 0 s: 1024000 batch_loss 0.6876479387283325 BPR_loss 0.6838343143463135 reg_loss 0.003813626244664192
epoch: 0 s: 1280000 batch_loss 0.6852461099624634 BPR_loss 0.6794055700302124 reg_loss 0.005840512923896313
+-------+-------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss        |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 184.4859058856964 | 1441.6809465885162 | 460.26580810546875 | [0.02535681 0.04563726 0.05989436] | [0.01131914 0.0155635  0.0181909 ] | [0.00143379 0.0013029  0.00115582] | [0.02842017 0.05133448 0.06803461] |
|   0   | 184.4859058856964 | 1423.6696255207062 | 460.26580810546875 | [0.04365549 0.06051509 0.07545075] | [0.01790109 0.02146164 0.0241943 ] | [0.0025546  0.00177483 0.00147971] | [0.05021745 0.06929125 0.08634029] |
+-------+-------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6799103617668152 BPR_loss 0.6730241775512695 reg_loss 0.006886158604174852
epoch: 1 s: 256000 batch_loss 0.6827658414840698 BPR_loss 0.6749734282493591 reg_loss 0.007792397402226925
epoch: 1 s: 512000 batch_loss 0.6800900101661682 BPR_loss 0.6704747080802917 reg_loss 0.009615326300263405
epoch: 1 s: 768000 batch_loss 0.6776039004325867 BPR_loss 0.6670786142349243 reg_loss 0.01052525918930769
epoch: 1 s: 1024000 batch_loss 0.6759504079818726 BPR_loss 0.6644180417060852 reg_loss 0.011532370932400227
epoch: 1 s: 1280000 batch_loss 0.675682783126831 BPR_loss 0.662684440612793 reg_loss 0.012998339720070362
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 126.36858654022217 | 1664.7261056900024 | 452.2171936035156 | [0.03117041 0.05211707 0.06692243] | [0.0144951  0.01889963 0.0216079 ] | [0.00177297 0.00150097 0.00129091] | [0.03504327 0.05899449 0.07584067] |
|   1   | 126.36858654022217 | 1377.6227197647095 | 452.2171936035156 | [0.04574652 0.06433106 0.07967345] | [0.01917164 0.02309129 0.02591049] | [0.00265426 0.00187567 0.00155902] | [0.05222646 0.0733014  0.09088617] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6741988062858582 BPR_loss 0.6613260507583618 reg_loss 0.012872782535851002
epoch: 2 s: 256000 batch_loss 0.672730028629303 BPR_loss 0.6592804193496704 reg_loss 0.013449585996568203
epoch: 2 s: 512000 batch_loss 0.6722394227981567 BPR_loss 0.6583441495895386 reg_loss 0.013895261101424694
epoch: 2 s: 768000 batch_loss 0.6681039333343506 BPR_loss 0.6541603803634644 reg_loss 0.013943544588983059
epoch: 2 s: 1024000 batch_loss 0.6673030853271484 BPR_loss 0.6513697504997253 reg_loss 0.015933319926261902
epoch: 2 s: 1280000 batch_loss 0.6645582318305969 BPR_loss 0.6482510566711426 reg_loss 0.01630718819797039
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 120.3504524230957 | 1594.0469324588776 | 447.0449523925781 | [0.03461657 0.05527042 0.07010706] | [0.01616801 0.02052224 0.02323711] | [0.00197013 0.00159517 0.0013548 ] | [0.03889883 0.06253605 0.07933112] |
|   2   | 120.3504524230957 | 1398.6353433132172 | 447.0449523925781 | [0.04795164 0.06860789 0.08500966] | [0.02040171 0.02475417 0.02776577] | [0.00277165 0.00199247 0.00165645] | [0.05455061 0.07789455 0.09662959] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6634213328361511 BPR_loss 0.6457158327102661 reg_loss 0.017705490812659264
epoch: 3 s: 256000 batch_loss 0.6596986055374146 BPR_loss 0.6425060033798218 reg_loss 0.017192602157592773
epoch: 3 s: 512000 batch_loss 0.6632746458053589 BPR_loss 0.6453608274459839 reg_loss 0.017913803458213806
epoch: 3 s: 768000 batch_loss 0.6651806831359863 BPR_loss 0.6479981541633606 reg_loss 0.01718253642320633
epoch: 3 s: 1024000 batch_loss 0.6637805700302124 BPR_loss 0.6459031105041504 reg_loss 0.017877433449029922
epoch: 3 s: 1280000 batch_loss 0.6633393168449402 BPR_loss 0.6453676819801331 reg_loss 0.017971646040678024
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 117.38681888580322 | 1705.4717683792114 | 443.0691833496094 | [0.03668716 0.05764329 0.07228739] | [0.01736962 0.02178573 0.02446778] | [0.00209756 0.00166362 0.00139813] | [0.04134507 0.06509913 0.08180656] |
|   3   | 117.38681888580322 | 1413.8615219593048 | 443.0691833496094 | [0.04962369 0.07154748 0.08784194] | [0.02117633 0.02580186 0.0287861 ] | [0.00286501 0.00207637 0.00170766] | [0.05637842 0.08123503 0.09971007] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6626259088516235 BPR_loss 0.6440457701683044 reg_loss 0.018580129370093346
epoch: 4 s: 256000 batch_loss 0.6578168272972107 BPR_loss 0.6369273662567139 reg_loss 0.02088947407901287
epoch: 4 s: 512000 batch_loss 0.6573366522789001 BPR_loss 0.63848876953125 reg_loss 0.01884789951145649
epoch: 4 s: 768000 batch_loss 0.6594821214675903 BPR_loss 0.6394661068916321 reg_loss 0.020016029477119446
epoch: 4 s: 1024000 batch_loss 0.6531360149383545 BPR_loss 0.6319124698638916 reg_loss 0.0212235264480114
epoch: 4 s: 1280000 batch_loss 0.6583964824676514 BPR_loss 0.6385347843170166 reg_loss 0.019861681386828423
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 119.76948857307434 | 1710.3966805934906 | 439.6822204589844 | [0.03818349 0.0591783  0.07494291] | [0.01818121 0.02260369 0.02548231] | [0.00217569 0.00170579 0.00144498] | [0.0427982  0.06672022 0.08453029] |
|   4   | 119.76948857307434 | 1419.3530201911926 | 439.6822204589844 | [0.05111397 0.07338425 0.09085514] | [0.02184131 0.02654197 0.02973847] | [0.00294655 0.00212542 0.00176123] | [0.05801714 0.08322041 0.10290086] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6558137536048889 BPR_loss 0.6354225873947144 reg_loss 0.02039114385843277
epoch: 5 s: 256000 batch_loss 0.6538625955581665 BPR_loss 0.6313012838363647 reg_loss 0.022561339661478996
epoch: 5 s: 512000 batch_loss 0.6558141112327576 BPR_loss 0.6332217454910278 reg_loss 0.022592337802052498
epoch: 5 s: 768000 batch_loss 0.6524870991706848 BPR_loss 0.6305456757545471 reg_loss 0.02194141410291195
epoch: 5 s: 1024000 batch_loss 0.650184690952301 BPR_loss 0.627561092376709 reg_loss 0.022623583674430847
epoch: 5 s: 1280000 batch_loss 0.6497228145599365 BPR_loss 0.6280251741409302 reg_loss 0.0216976348310709
