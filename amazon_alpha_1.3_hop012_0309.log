Namespace(K=1, Ks='[20, 40, 60]', alpha=1.3, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=4, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.693147599697113 BPR_loss 0.6931474208831787 reg_loss 1.6976315464489744e-07
epoch: 0 s: 256000 batch_loss 0.6930312514305115 BPR_loss 0.692974865436554 reg_loss 5.638591028400697e-05
epoch: 0 s: 512000 batch_loss 0.6908910870552063 BPR_loss 0.6896752119064331 reg_loss 0.001215847092680633
epoch: 0 s: 768000 batch_loss 0.6862673163414001 BPR_loss 0.6821689009666443 reg_loss 0.004098398145288229
epoch: 0 s: 1024000 batch_loss 0.6830161213874817 BPR_loss 0.6768519878387451 reg_loss 0.0061641051433980465
epoch: 0 s: 1280000 batch_loss 0.679455041885376 BPR_loss 0.6713166236877441 reg_loss 0.008138399571180344
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 184.91158437728882 | 1436.6471729278564 | 458.27667236328125 | [0.0259656  0.04697531 0.0612366 ] | [0.01175698 0.01614962 0.01877808] | [0.00147431 0.00134269 0.0011821 ] | [0.02921611 0.05294826 0.06962649] |
|   0   | 184.91158437728882 | 1283.735021829605  | 458.27667236328125 | [0.04317153 0.05959713 0.07457512] | [0.01766737 0.02113862 0.02387462] | [0.00251875 0.00174587 0.00145831] | [0.0496108  0.06820402 0.08516639] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6721010804176331 BPR_loss 0.663001537322998 reg_loss 0.009099570102989674
epoch: 1 s: 256000 batch_loss 0.6767363548278809 BPR_loss 0.6671414375305176 reg_loss 0.009594915434718132
epoch: 1 s: 512000 batch_loss 0.6737310886383057 BPR_loss 0.6624042391777039 reg_loss 0.01132685411721468
epoch: 1 s: 768000 batch_loss 0.6712957620620728 BPR_loss 0.6593177318572998 reg_loss 0.011978010646998882
epoch: 1 s: 1024000 batch_loss 0.6685031056404114 BPR_loss 0.6556973457336426 reg_loss 0.012805759906768799
epoch: 1 s: 1280000 batch_loss 0.668125569820404 BPR_loss 0.6539335250854492 reg_loss 0.014192024245858192
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 117.45112752914429 | 1544.4224770069122 | 447.5030822753906 | [0.03210664 0.05303973 0.06765076] | [0.01514798 0.01955398 0.02222598] | [0.00183103 0.00153145 0.001306  ] | [0.03617511 0.06024316 0.07674614] |
|   1   | 117.45112752914429 | 1467.112559080124  | 447.5030822753906 | [0.04551071 0.06404914 0.07937087] | [0.01907649 0.02297811 0.02579457] | [0.00263732 0.00186523 0.00155206] | [0.05193496 0.07299414 0.09052376] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6662865877151489 BPR_loss 0.6522746682167053 reg_loss 0.014011901803314686
epoch: 2 s: 256000 batch_loss 0.664945662021637 BPR_loss 0.6504705548286438 reg_loss 0.014475105330348015
epoch: 2 s: 512000 batch_loss 0.6642181277275085 BPR_loss 0.6494584083557129 reg_loss 0.014759717509150505
epoch: 2 s: 768000 batch_loss 0.6593318581581116 BPR_loss 0.6446362733840942 reg_loss 0.014695573598146439
epoch: 2 s: 1024000 batch_loss 0.6582522988319397 BPR_loss 0.6416231393814087 reg_loss 0.016629163175821304
epoch: 2 s: 1280000 batch_loss 0.6548764705657959 BPR_loss 0.6378198862075806 reg_loss 0.017056556418538094
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 110.41029787063599 | 1565.7556641101837 | 441.2392272949219 | [0.03538142 0.05625619 0.07052662] | [0.01676508 0.02116537 0.0237765 ] | [0.00201723 0.00162383 0.00136235] | [0.03985542 0.06371171 0.07985688] |
|   2   | 110.41029787063599 | 1401.0932033061981 | 441.2392272949219 | [0.04749432 0.06849496 0.08471713] | [0.02031132 0.02474015 0.02771779] | [0.00274447 0.00198715 0.00164897] | [0.05399912 0.0777685  0.09628293] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6529892683029175 BPR_loss 0.634466290473938 reg_loss 0.018522989004850388
epoch: 3 s: 256000 batch_loss 0.6486069560050964 BPR_loss 0.6307848691940308 reg_loss 0.01782209239900112
epoch: 3 s: 512000 batch_loss 0.6530011892318726 BPR_loss 0.6345242261886597 reg_loss 0.01847698539495468
epoch: 3 s: 768000 batch_loss 0.6554585099220276 BPR_loss 0.6377741098403931 reg_loss 0.01768440380692482
epoch: 3 s: 1024000 batch_loss 0.6528841257095337 BPR_loss 0.6344040632247925 reg_loss 0.018480034545063972
epoch: 3 s: 1280000 batch_loss 0.653040885925293 BPR_loss 0.6346267461776733 reg_loss 0.01841410994529724
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 107.2034502029419 | 1524.2127966880798 | 436.1496887207031 | [0.03751231 0.05831344 0.0730985 ] | [0.01791382 0.02230697 0.02501369] | [0.0021432  0.00168243 0.00141237] | [0.04227245 0.06585856 0.08270474] |
|   3   | 107.2034502029419 | 1434.824690580368  | 436.1496887207031 | [0.04961705 0.07170964 0.08846121] | [0.02123503 0.02589435 0.028963  ] | [0.00286265 0.00207677 0.00171935] | [0.05632327 0.08136109 0.10044277] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6515976190567017 BPR_loss 0.6325450539588928 reg_loss 0.019052566960453987
epoch: 4 s: 256000 batch_loss 0.6453495621681213 BPR_loss 0.6241007447242737 reg_loss 0.0212488304823637
epoch: 4 s: 512000 batch_loss 0.6445701718330383 BPR_loss 0.6251846551895142 reg_loss 0.01938551478087902
epoch: 4 s: 768000 batch_loss 0.6473576426506042 BPR_loss 0.6268666386604309 reg_loss 0.020491015166044235
epoch: 4 s: 1024000 batch_loss 0.6389821171760559 BPR_loss 0.6174083948135376 reg_loss 0.021573709324002266
epoch: 4 s: 1280000 batch_loss 0.646308958530426 BPR_loss 0.6259599924087524 reg_loss 0.020348986610770226
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 112.8246500492096 | 1450.6835339069366 | 431.6241455078125 | [0.03917643 0.0602857  0.0759158 ] | [0.01872466 0.02316662 0.02602006] | [0.00223338 0.00173409 0.00146081] | [0.04398846 0.06785936 0.08549418] |
|   4   | 112.8246500492096 | 1257.239673614502  | 431.6241455078125 | [0.05118958 0.07375342 0.09120649] | [0.02195968 0.02671531 0.02991632] | [0.00294734 0.00213014 0.00176596] | [0.05806441 0.0834804  0.10324751] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6425791382789612 BPR_loss 0.6217101812362671 reg_loss 0.020868949592113495
epoch: 5 s: 256000 batch_loss 0.6400348544120789 BPR_loss 0.6170158982276917 reg_loss 0.023018941283226013
epoch: 5 s: 512000 batch_loss 0.6417431831359863 BPR_loss 0.6187010407447815 reg_loss 0.02304215170443058
epoch: 5 s: 768000 batch_loss 0.6383946537971497 BPR_loss 0.616075336933136 reg_loss 0.022319341078400612
epoch: 5 s: 1024000 batch_loss 0.6350076794624329 BPR_loss 0.6120939254760742 reg_loss 0.022913767024874687
epoch: 5 s: 1280000 batch_loss 0.6341991424560547 BPR_loss 0.6121183633804321 reg_loss 0.02208074927330017
