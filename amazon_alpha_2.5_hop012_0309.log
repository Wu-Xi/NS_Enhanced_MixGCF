Namespace(K=1, Ks='[20, 40, 60]', alpha=2.5, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=3, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.6931474804878235 BPR_loss 0.6931473016738892 reg_loss 1.7449950462378183e-07
epoch: 0 s: 256000 batch_loss 0.6931407451629639 BPR_loss 0.6931361556053162 reg_loss 4.613398232322652e-06
epoch: 0 s: 512000 batch_loss 0.692936360836029 BPR_loss 0.6928116083145142 reg_loss 0.00012477976270020008
epoch: 0 s: 768000 batch_loss 0.6916738748550415 BPR_loss 0.6907267570495605 reg_loss 0.0009471407975070179
epoch: 0 s: 1024000 batch_loss 0.6898143887519836 BPR_loss 0.6874363422393799 reg_loss 0.0023780204355716705
epoch: 0 s: 1280000 batch_loss 0.6880895495414734 BPR_loss 0.6839509010314941 reg_loss 0.00413866713643074
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 117.91645288467407 | 1449.2424659729004 | 461.1097412109375 | [0.02509496 0.04501811 0.05917577] | [0.01108573 0.01525875 0.01785736] | [0.00141772 0.00128519 0.00113853] | [0.02807697 0.05064077 0.06702691] |
|   0   | 117.91645288467407 | 1406.8685369491577 | 461.1097412109375 | [0.0439722  0.06133707 0.0762573 ] | [0.01803473 0.02169777 0.02442847] | [0.00257666 0.00180122 0.00149862] | [0.05057986 0.07026818 0.08733298] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6840500831604004 BPR_loss 0.6789302825927734 reg_loss 0.005119808949530125
epoch: 1 s: 256000 batch_loss 0.6858465671539307 BPR_loss 0.679649829864502 reg_loss 0.0061967456713318825
epoch: 1 s: 512000 batch_loss 0.6835681796073914 BPR_loss 0.6755761504173279 reg_loss 0.007992028258740902
epoch: 1 s: 768000 batch_loss 0.6809948086738586 BPR_loss 0.6719269752502441 reg_loss 0.009067821316421032
epoch: 1 s: 1024000 batch_loss 0.6800382733345032 BPR_loss 0.6698850393295288 reg_loss 0.010153260082006454
epoch: 1 s: 1280000 batch_loss 0.6797508001327515 BPR_loss 0.6680983304977417 reg_loss 0.011652464047074318
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 102.89524483680725 | 1675.2572910785675 | 454.76434326171875 | [0.03087276 0.05156957 0.06638114] | [0.0142145  0.01855983 0.02127099] | [0.00175764 0.00148326 0.00127983] | [0.03472197 0.05827887 0.07517617] |
|   1   | 102.89524483680725 | 1359.0884220600128 | 454.76434326171875 | [0.0461981  0.06469711 0.08000757] | [0.01932671 0.02322651 0.02603894] | [0.0026838  0.0018867  0.00156572] | [0.05280159 0.07369532 0.09124858] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6784672141075134 BPR_loss 0.6668714284896851 reg_loss 0.01159579586237669
epoch: 2 s: 256000 batch_loss 0.6769112944602966 BPR_loss 0.664650559425354 reg_loss 0.012260734103620052
epoch: 2 s: 512000 batch_loss 0.6766106486320496 BPR_loss 0.6637911200523376 reg_loss 0.012819542549550533
epoch: 2 s: 768000 batch_loss 0.6729534864425659 BPR_loss 0.6599748134613037 reg_loss 0.012978699989616871
epoch: 2 s: 1024000 batch_loss 0.6721561551094055 BPR_loss 0.6572134494781494 reg_loss 0.014942707493901253
epoch: 2 s: 1280000 batch_loss 0.6697770357131958 BPR_loss 0.6544662714004517 reg_loss 0.01531077641993761
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 98.19206261634827 | 1570.7859201431274 | 450.1676330566406 | [0.0342886  0.05494465 0.06965718] | [0.0159258  0.02026877 0.0229654 ] | [0.00195224 0.0015833  0.00134677] | [0.03852642 0.0620322  0.07885648] |
|   2   | 98.19206261634827 | 1371.5323295593262 | 450.1676330566406 | [0.04805203 0.06888157 0.08529089] | [0.02040105 0.02479139 0.02779962] | [0.00278307 0.0020033  0.00166249] | [0.05473182 0.07820182 0.09698412] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6690447330474854 BPR_loss 0.6524293422698975 reg_loss 0.016615360975265503
epoch: 3 s: 256000 batch_loss 0.6657227873802185 BPR_loss 0.6494626402854919 reg_loss 0.01626014895737171
epoch: 3 s: 512000 batch_loss 0.6687111854553223 BPR_loss 0.6516798734664917 reg_loss 0.017031319439411163
epoch: 3 s: 768000 batch_loss 0.6703123450279236 BPR_loss 0.6539303064346313 reg_loss 0.01638204976916313
epoch: 3 s: 1024000 batch_loss 0.6694566607475281 BPR_loss 0.652428388595581 reg_loss 0.017028281465172768
epoch: 3 s: 1280000 batch_loss 0.6687796115875244 BPR_loss 0.651576817035675 reg_loss 0.017202774062752724
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 107.19008183479309 | 1526.2622394561768 | 446.7275695800781 | [0.03624235 0.05715724 0.07186411] | [0.0170637  0.02146685 0.02416148] | [0.00207419 0.00164847 0.001389  ] | [0.0408047  0.06445653 0.0812443 ] |
|   3   | 107.19008183479309 | 1401.2641379833221 | 446.7275695800781 | [0.04948822 0.07149974 0.08790135] | [0.0211493  0.02579759 0.02880769] | [0.0028595  0.002075   0.00171055] | [0.05623661 0.08114837 0.09987552] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6684997081756592 BPR_loss 0.6506972312927246 reg_loss 0.017802471294999123
epoch: 4 s: 256000 batch_loss 0.6643935441970825 BPR_loss 0.6443092823028564 reg_loss 0.020084258168935776
epoch: 4 s: 512000 batch_loss 0.6638864874839783 BPR_loss 0.6458258628845215 reg_loss 0.018060605973005295
epoch: 4 s: 768000 batch_loss 0.6658740043640137 BPR_loss 0.6466171741485596 reg_loss 0.019256846979260445
epoch: 4 s: 1024000 batch_loss 0.6606414318084717 BPR_loss 0.6401639580726624 reg_loss 0.020477494224905968
epoch: 4 s: 1280000 batch_loss 0.6646019220352173 BPR_loss 0.6454662680625916 reg_loss 0.019135626032948494
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 117.23436284065247 | 1589.9051642417908 | 443.8725891113281 | [0.03764016 0.05883199 0.07414976] | [0.01787706 0.02233076 0.02513358] | [0.00214794 0.0016932  0.00142965] | [0.04221403 0.06619446 0.08360291] |
|   4   | 117.23436284065247 | 1437.5781474113464 | 443.8725891113281 | [0.05096501 0.07347591 0.09081067] | [0.02172894 0.02647665 0.02964649] | [0.00294104 0.00213014 0.00176137] | [0.05789109 0.08330707 0.10286934] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6625325679779053 BPR_loss 0.6428833603858948 reg_loss 0.0196492038667202
epoch: 5 s: 256000 batch_loss 0.6609013080596924 BPR_loss 0.6391221284866333 reg_loss 0.021779190748929977
epoch: 5 s: 512000 batch_loss 0.6629165410995483 BPR_loss 0.6411036252975464 reg_loss 0.021812917664647102
epoch: 5 s: 768000 batch_loss 0.6597943902015686 BPR_loss 0.638534665107727 reg_loss 0.021259721368551254
epoch: 5 s: 1024000 batch_loss 0.6580636501312256 BPR_loss 0.6360959410667419 reg_loss 0.021967733278870583
epoch: 5 s: 1280000 batch_loss 0.6578038930892944 BPR_loss 0.6367601156234741 reg_loss 0.021043788641691208
