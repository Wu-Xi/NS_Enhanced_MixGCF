Namespace(K=1, Ks='[20, 40, 60]', alpha=1.9, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=4, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.693147599697113 BPR_loss 0.6931474208831787 reg_loss 1.717858282290763e-07
epoch: 0 s: 256000 batch_loss 0.6931226849555969 BPR_loss 0.693108320236206 reg_loss 1.4368490155902691e-05
epoch: 0 s: 512000 batch_loss 0.6924210786819458 BPR_loss 0.6920086741447449 reg_loss 0.0004124002007301897
epoch: 0 s: 768000 batch_loss 0.6897748112678528 BPR_loss 0.687628984451294 reg_loss 0.002145821927115321
epoch: 0 s: 1024000 batch_loss 0.6871089935302734 BPR_loss 0.682965874671936 reg_loss 0.004143129102885723
epoch: 0 s: 1280000 batch_loss 0.6845550537109375 BPR_loss 0.6783612370491028 reg_loss 0.006193842738866806
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 184.89830875396729 | 1426.3425390720367 | 460.0452575683594 | [0.02544151 0.04590961 0.06010852] | [0.01137503 0.01565393 0.01826825] | [0.00143926 0.00131075 0.00115959] | [0.0285297  0.05161196 0.06827559] |
|   0   | 184.89830875396729 | 1307.3645446300507 | 460.0452575683594 | [0.04367407 0.06041194 0.07530084] | [0.01789104 0.02142447 0.02414752] | [0.00255302 0.00177148 0.00147643] | [0.0502332  0.06919671 0.08613545] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6789591312408447 BPR_loss 0.6717187166213989 reg_loss 0.007240417879074812
epoch: 1 s: 256000 batch_loss 0.6820389628410339 BPR_loss 0.6739449501037598 reg_loss 0.008093993179500103
epoch: 1 s: 512000 batch_loss 0.6792863011360168 BPR_loss 0.6693693995475769 reg_loss 0.009916889481246471
epoch: 1 s: 768000 batch_loss 0.6767973303794861 BPR_loss 0.6660078763961792 reg_loss 0.010789441876113415
epoch: 1 s: 1024000 batch_loss 0.6750267148017883 BPR_loss 0.6632497310638428 reg_loss 0.011776959523558617
epoch: 1 s: 1280000 batch_loss 0.6747528314590454 BPR_loss 0.6615280508995056 reg_loss 0.013224759139120579
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)  |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 116.59738421440125 | 1563.184520483017 | 451.6287841796875 | [0.03117602 0.05227133 0.06706424] | [0.01456426 0.01900377 0.02170935] | [0.00177407 0.0015059  0.00129383] | [0.03505787 0.05921355 0.07599401] |
|   1   | 116.59738421440125 | 1332.405276298523 | 451.6287841796875 | [0.0457779  0.06435011 0.0795531 ] | [0.01917003 0.02308216 0.02587658] | [0.00265623 0.00187587 0.00155666] | [0.05228161 0.07332503 0.09075224] |
+-------+--------------------+-------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6732114553451538 BPR_loss 0.6601231098175049 reg_loss 0.013088329695165157
epoch: 2 s: 256000 batch_loss 0.6717612743377686 BPR_loss 0.6581188440322876 reg_loss 0.013642403297126293
epoch: 2 s: 512000 batch_loss 0.6712103486061096 BPR_loss 0.657146692276001 reg_loss 0.014063673093914986
epoch: 2 s: 768000 batch_loss 0.6670593023300171 BPR_loss 0.6529641151428223 reg_loss 0.014095185324549675
epoch: 2 s: 1024000 batch_loss 0.6661882996559143 BPR_loss 0.6501109600067139 reg_loss 0.016077328473329544
epoch: 2 s: 1280000 batch_loss 0.6633890271186829 BPR_loss 0.6469342112541199 reg_loss 0.016454819589853287
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 116.67892146110535 | 1465.2024297714233 | 446.3355712890625 | [0.03473262 0.05546859 0.07009367] | [0.01626155 0.02062933 0.02330646] | [0.00197671 0.00160046 0.0013548 ] | [0.03903757 0.06276973 0.07933842] |
|   2   | 116.67892146110535 | 1277.384225845337  | 446.3355712890625 | [0.04791853 0.06832644 0.08478315] | [0.02036312 0.02467076 0.02769284] | [0.00276929 0.00198577 0.00165212] | [0.05449546 0.0776582  0.09640111] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6621658802032471 BPR_loss 0.6442931294441223 reg_loss 0.017872760072350502
epoch: 3 s: 256000 batch_loss 0.6583983898162842 BPR_loss 0.6410694122314453 reg_loss 0.017328966408967972
epoch: 3 s: 512000 batch_loss 0.662056565284729 BPR_loss 0.6440158486366272 reg_loss 0.018040714785456657
epoch: 3 s: 768000 batch_loss 0.6639890670776367 BPR_loss 0.6467013359069824 reg_loss 0.017287738621234894
epoch: 3 s: 1024000 batch_loss 0.6624792218208313 BPR_loss 0.6444827318191528 reg_loss 0.017996488139033318
epoch: 3 s: 1280000 batch_loss 0.6621330976486206 BPR_loss 0.6440619230270386 reg_loss 0.018071157857775688
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 108.05311036109924 | 1398.7581911087036 | 442.2459411621094 | [0.03672564 0.05776604 0.07238438] | [0.01741584 0.02184996 0.0245268 ] | [0.00210048 0.00166728 0.00139971] | [0.04140348 0.06525247 0.0819088 ] |
|   3   | 108.05311036109924 | 1298.1099462509155 | 442.2459411621094 | [0.04962763 0.07143587 0.08797568] | [0.02113201 0.02573422 0.0287614 ] | [0.00286422 0.00207322 0.00171002] | [0.05635478 0.0811011  0.09982825] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6613824963569641 BPR_loss 0.6427039504051208 reg_loss 0.018678562715649605
epoch: 4 s: 256000 batch_loss 0.6563560962677002 BPR_loss 0.6353686451911926 reg_loss 0.02098744921386242
epoch: 4 s: 512000 batch_loss 0.6557684540748596 BPR_loss 0.6368168592453003 reg_loss 0.018951572477817535
epoch: 4 s: 768000 batch_loss 0.6580857634544373 BPR_loss 0.6379643678665161 reg_loss 0.020121388137340546
epoch: 4 s: 1024000 batch_loss 0.6514190435409546 BPR_loss 0.6301095485687256 reg_loss 0.02130952477455139
epoch: 4 s: 1280000 batch_loss 0.6569541096687317 BPR_loss 0.6370012760162354 reg_loss 0.019952861592173576
+-------+--------------------+--------------------+------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |       Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 117.25538420677185 | 1549.7718861103058 | 438.739501953125 | [0.03828523 0.05931433 0.0749354 ] | [0.01821567 0.0226433  0.02549694] | [0.00218299 0.00170945 0.0014445 ] | [0.04293695 0.06686626 0.08450838] |
|   4   | 117.25538420677185 | 1471.4922018051147 | 438.739501953125 | [0.05111196 0.0733859  0.09059807] | [0.02183016 0.02653326 0.02968601] | [0.00294576 0.00212502 0.00175677] | [0.05800139 0.08321253 0.10267238] |
+-------+--------------------+--------------------+------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6542224884033203 BPR_loss 0.6337366700172424 reg_loss 0.02048581652343273
epoch: 5 s: 256000 batch_loss 0.6523237228393555 BPR_loss 0.6296665668487549 reg_loss 0.022657152265310287
epoch: 5 s: 512000 batch_loss 0.6542000770568848 BPR_loss 0.6315158009529114 reg_loss 0.022684305906295776
epoch: 5 s: 768000 batch_loss 0.6508525013923645 BPR_loss 0.6288286447525024 reg_loss 0.02202383428812027
epoch: 5 s: 1024000 batch_loss 0.6484366059303284 BPR_loss 0.6257388591766357 reg_loss 0.022697770968079567
epoch: 5 s: 1280000 batch_loss 0.6479101777076721 BPR_loss 0.6261346340179443 reg_loss 0.021775521337985992
