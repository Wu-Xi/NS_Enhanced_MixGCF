Namespace(K=1, Ks='[20, 40, 60]', alpha=2.3, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=3, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.6931474804878235 BPR_loss 0.6931473016738892 reg_loss 1.7359087678414653e-07
epoch: 0 s: 256000 batch_loss 0.6931374073028564 BPR_loss 0.6931308507919312 reg_loss 6.547570592374541e-06
epoch: 0 s: 512000 batch_loss 0.692825973033905 BPR_loss 0.6926388144493103 reg_loss 0.00018717345665208995
epoch: 0 s: 768000 batch_loss 0.6911842823028564 BPR_loss 0.6899186968803406 reg_loss 0.001265567960217595
epoch: 0 s: 1024000 batch_loss 0.6890525221824646 BPR_loss 0.6861442923545837 reg_loss 0.002908222610130906
epoch: 0 s: 1280000 batch_loss 0.6870800852775574 BPR_loss 0.6822775602340698 reg_loss 0.004802541807293892
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 118.56332683563232 | 1434.9562993049622 | 460.8169250488281 | [0.02531459 0.04519507 0.0595025 ] | [0.01119923 0.01535912 0.0179906 ] | [0.0014305  0.00129085 0.0011473 ] | [0.02833984 0.05083793 0.06753076] |
|   0   | 118.56332683563232 | 1423.1283888816833 | 460.8169250488281 | [0.04398943 0.06099085 0.07583787] | [0.01799524 0.02158463 0.0243011 ] | [0.00257429 0.00179078 0.00148916] | [0.05057986 0.0698585  0.08680512] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.682534396648407 BPR_loss 0.6767135262489319 reg_loss 0.005820861551910639
epoch: 1 s: 256000 batch_loss 0.6847193837165833 BPR_loss 0.6778720021247864 reg_loss 0.006847385782748461
epoch: 1 s: 512000 batch_loss 0.682285487651825 BPR_loss 0.6736198663711548 reg_loss 0.008665645495057106
epoch: 1 s: 768000 batch_loss 0.6796826124191284 BPR_loss 0.6700036525726318 reg_loss 0.009678932838141918
epoch: 1 s: 1024000 batch_loss 0.6785229444503784 BPR_loss 0.6677803993225098 reg_loss 0.010742522776126862
epoch: 1 s: 1280000 batch_loss 0.6782453060150146 BPR_loss 0.6660186052322388 reg_loss 0.012226673774421215
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 103.6941773891449 | 1473.529773235321  | 453.8173522949219 | [0.03087257 0.05172488 0.06661804] | [0.01430664 0.01869058 0.02141866] | [0.00175691 0.00148819 0.00128555] | [0.03472197 0.05848333 0.07546095] |
|   1   | 103.6941773891449 | 1441.9106061458588 | 453.8173522949219 | [0.04607496 0.06451749 0.08001224] | [0.01929882 0.02318916 0.02603671] | [0.00267238 0.00188059 0.00156637] | [0.05258887 0.07350624 0.09128797] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6768823862075806 BPR_loss 0.6647307872772217 reg_loss 0.012151619419455528
epoch: 2 s: 256000 batch_loss 0.6753321886062622 BPR_loss 0.6625581383705139 reg_loss 0.012774077244102955
epoch: 2 s: 512000 batch_loss 0.6749677062034607 BPR_loss 0.6616824269294739 reg_loss 0.013285279273986816
epoch: 2 s: 768000 batch_loss 0.6711193323135376 BPR_loss 0.6577147245407104 reg_loss 0.013404608704149723
epoch: 2 s: 1024000 batch_loss 0.6703259944915771 BPR_loss 0.6549392342567444 reg_loss 0.015386790037155151
epoch: 2 s: 1280000 batch_loss 0.6678164601325989 BPR_loss 0.6520635485649109 reg_loss 0.015752898529171944
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 96.50884509086609 | 1554.5753865242004 | 449.0023498535156 | [0.03453612 0.05510633 0.06995179] | [0.01605563 0.02038048 0.02309976] | [0.00196575 0.00158805 0.00135164] | [0.0387966  0.06224397 0.07917047] |
|   2   | 96.50884509086609 | 1430.9786908626556 | 449.0023498535156 | [0.04801857 0.06868807 0.08519477] | [0.02037198 0.02473297 0.02776012] | [0.00277717 0.00199798 0.00166039] | [0.0546294  0.07805212 0.09687382] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6669532060623169 BPR_loss 0.6498486995697021 reg_loss 0.0171045009046793
epoch: 3 s: 256000 batch_loss 0.6634745597839355 BPR_loss 0.6467961072921753 reg_loss 0.016678480431437492
epoch: 3 s: 512000 batch_loss 0.6666760444641113 BPR_loss 0.649256706237793 reg_loss 0.017419351264834404
epoch: 3 s: 768000 batch_loss 0.668393075466156 BPR_loss 0.6516544818878174 reg_loss 0.016738571226596832
epoch: 3 s: 1024000 batch_loss 0.6673471331596375 BPR_loss 0.6499404907226562 reg_loss 0.01740666665136814
epoch: 3 s: 1280000 batch_loss 0.6667736172676086 BPR_loss 0.649224042892456 reg_loss 0.01754956692457199
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 114.35578274726868 | 1448.8425831794739 | 445.3680725097656 | [0.03635237 0.05727991 0.07206677] | [0.01715577 0.02156394 0.02427508] | [0.0020804  0.00165158 0.00139314] | [0.04095075 0.06459528 0.08150717] |
|   3   | 114.35578274726868 | 1238.7303013801575 | 445.3680725097656 | [0.04953975 0.07144905 0.08775894] | [0.02114086 0.02576735 0.02875865] | [0.00286225 0.00207401 0.00170727] | [0.05630751 0.08110898 0.09964704] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6662998199462891 BPR_loss 0.6481484174728394 reg_loss 0.018151378259062767
epoch: 4 s: 256000 batch_loss 0.6619457602500916 BPR_loss 0.6414884328842163 reg_loss 0.020457332953810692
epoch: 4 s: 512000 batch_loss 0.6614684462547302 BPR_loss 0.6430529356002808 reg_loss 0.018415534868836403
epoch: 4 s: 768000 batch_loss 0.6635106801986694 BPR_loss 0.6439077258110046 reg_loss 0.019602928310632706
epoch: 4 s: 1024000 batch_loss 0.6577702164649963 BPR_loss 0.6369516849517822 reg_loss 0.020818544551730156
epoch: 4 s: 1280000 batch_loss 0.662298321723938 BPR_loss 0.6428415775299072 reg_loss 0.01945674978196621
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 124.18465423583984 | 1364.4579482078552 | 442.3232116699219 | [0.03780018 0.05900984 0.07434861] | [0.01797198 0.02243279 0.02523929] | [0.00215817 0.00170014 0.00143476] | [0.04242579 0.06647194 0.08388769] |
|   4   | 124.18465423583984 | 1249.6488423347473 | 442.3232116699219 | [0.05107165 0.07316048 0.09093038] | [0.02179785 0.02646331 0.02971261] | [0.00294458 0.00212089 0.00176307] | [0.05798563 0.08299981 0.10297964] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6600618958473206 BPR_loss 0.6400823593139648 reg_loss 0.019979529082775116
epoch: 5 s: 256000 batch_loss 0.658328652381897 BPR_loss 0.636194109916687 reg_loss 0.0221345704048872
epoch: 5 s: 512000 batch_loss 0.6603005528450012 BPR_loss 0.6381291151046753 reg_loss 0.022171437740325928
epoch: 5 s: 768000 batch_loss 0.6571139693260193 BPR_loss 0.6355428695678711 reg_loss 0.02157111093401909
epoch: 5 s: 1024000 batch_loss 0.655190110206604 BPR_loss 0.632920503616333 reg_loss 0.0222695991396904
epoch: 5 s: 1280000 batch_loss 0.6548402905464172 BPR_loss 0.6335006356239319 reg_loss 0.021339651197195053
