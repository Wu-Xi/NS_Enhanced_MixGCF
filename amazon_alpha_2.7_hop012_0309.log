Namespace(K=1, Ks='[20, 40, 60]', alpha=2.7, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=3, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.6931474804878235 BPR_loss 0.6931473016738892 reg_loss 1.753894025569025e-07
epoch: 0 s: 256000 batch_loss 0.693142831325531 BPR_loss 0.6931394934654236 reg_loss 3.33896809934231e-06
epoch: 0 s: 512000 batch_loss 0.6930087804794312 BPR_loss 0.6929259300231934 reg_loss 8.282577618956566e-05
epoch: 0 s: 768000 batch_loss 0.6920539736747742 BPR_loss 0.6913545727729797 reg_loss 0.0006993965362198651
epoch: 0 s: 1024000 batch_loss 0.6904438734054565 BPR_loss 0.6885288953781128 reg_loss 0.0019149521831423044
epoch: 0 s: 1280000 batch_loss 0.6889727115631104 BPR_loss 0.6854596138000488 reg_loss 0.0035131252370774746
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 116.96252369880676 | 1456.3706102371216 | 461.3507995605469 | [0.02501975 0.0447951  0.05892568] | [0.01102353 0.01516564 0.017754  ] | [0.00141517 0.00127843 0.00113123] | [0.02802585 0.05033408 0.06661799] |
|   0   | 116.96252369880676 | 1284.869090795517  | 461.3507995605469 | [0.04389914 0.06150882 0.07633696] | [0.0180637  0.02178213 0.024499  ] | [0.00257508 0.00180713 0.00150111] | [0.05053259 0.0704809  0.08747479] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6853911280632019 BPR_loss 0.68094801902771 reg_loss 0.004443108104169369
epoch: 1 s: 256000 batch_loss 0.6868566274642944 BPR_loss 0.6813164949417114 reg_loss 0.005540121346712112
epoch: 1 s: 512000 batch_loss 0.6847490668296814 BPR_loss 0.6774520874023438 reg_loss 0.007297007832676172
epoch: 1 s: 768000 batch_loss 0.6821441054344177 BPR_loss 0.6737209558486938 reg_loss 0.008423150517046452
epoch: 1 s: 1024000 batch_loss 0.6813986301422119 BPR_loss 0.6718723773956299 reg_loss 0.009526234120130539
epoch: 1 s: 1280000 batch_loss 0.6811299324035645 BPR_loss 0.6700985431671143 reg_loss 0.011031403206288815
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 88.97179222106934 | 1386.601099729538  | 455.6223449707031 | [0.03069933 0.05141113 0.06637968] | [0.01411058 0.01846009 0.02119753] | [0.00174888 0.00147906 0.00127971] | [0.03452481 0.05811822 0.07519077] |
|   1   | 88.97179222106934 | 1331.0698125362396 | 455.6223449707031 | [0.04620985 0.06478978 0.080244  ] | [0.01932813 0.02324763 0.02608816] | [0.00268617 0.00188965 0.00157097] | [0.05281735 0.07376623 0.09154009] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6798906922340393 BPR_loss 0.6688939929008484 reg_loss 0.01099669374525547
epoch: 2 s: 256000 batch_loss 0.6782870292663574 BPR_loss 0.6665838956832886 reg_loss 0.011703118681907654
epoch: 2 s: 512000 batch_loss 0.6780931353569031 BPR_loss 0.6657850742340088 reg_loss 0.01230804156512022
epoch: 2 s: 768000 batch_loss 0.6745491027832031 BPR_loss 0.6620365381240845 reg_loss 0.012512543238699436
epoch: 2 s: 1024000 batch_loss 0.6738782525062561 BPR_loss 0.6594197750091553 reg_loss 0.014458480291068554
epoch: 2 s: 1280000 batch_loss 0.6715532541275024 BPR_loss 0.6567291021347046 reg_loss 0.014824130572378635
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 120.63782739639282 | 1478.6612358093262 | 451.2302551269531 | [0.03417152 0.05481506 0.06950959] | [0.01584607 0.02018746 0.02288051] | [0.00194713 0.00157965 0.00134361] | [0.03841688 0.06188616 0.07866662] |
|   2   | 120.63782739639282 | 1401.720021724701  | 451.2302551269531 | [0.04812852 0.0689656  0.08543754] | [0.02040083 0.02479587 0.02781292] | [0.00278741 0.00200645 0.00166459] | [0.05485787 0.07831211 0.09711017] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6709513068199158 BPR_loss 0.654875636100769 reg_loss 0.016075661405920982
epoch: 3 s: 256000 batch_loss 0.6677843928337097 BPR_loss 0.651989221572876 reg_loss 0.015795154497027397
epoch: 3 s: 512000 batch_loss 0.6705387830734253 BPR_loss 0.6539509296417236 reg_loss 0.016587883234024048
epoch: 3 s: 768000 batch_loss 0.6720829606056213 BPR_loss 0.6561037302017212 reg_loss 0.015979250892996788
epoch: 3 s: 1024000 batch_loss 0.6714172959327698 BPR_loss 0.6548100113868713 reg_loss 0.016607293859124184
epoch: 3 s: 1280000 batch_loss 0.6706337928771973 BPR_loss 0.653822660446167 reg_loss 0.01681114360690117
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss        |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 103.75747752189636 | 1617.7093641757965 | 447.96624755859375 | [0.03603337 0.05707541 0.0716636 ] | [0.01699516 0.02142614 0.02409991] | [0.00206506 0.00164628 0.00138571] | [0.04058564 0.06437621 0.08098142] |
|   3   | 103.75747752189636 | 1490.6666100025177 | 447.96624755859375 | [0.04955493 0.07143942 0.08799925] | [0.02114681 0.02576544 0.02880717] | [0.00286501 0.00207224 0.00171265] | [0.05632327 0.08106958 0.09997006] |
+-------+--------------------+--------------------+--------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6704277992248535 BPR_loss 0.6530172824859619 reg_loss 0.01741054467856884
epoch: 4 s: 256000 batch_loss 0.6666353940963745 BPR_loss 0.6469811797142029 reg_loss 0.01965423859655857
epoch: 4 s: 512000 batch_loss 0.6661132574081421 BPR_loss 0.6484449505805969 reg_loss 0.017668284475803375
epoch: 4 s: 768000 batch_loss 0.6680428385734558 BPR_loss 0.6491773128509521 reg_loss 0.018865538761019707
epoch: 4 s: 1024000 batch_loss 0.6631612777709961 BPR_loss 0.643085777759552 reg_loss 0.02007550746202469
epoch: 4 s: 1280000 batch_loss 0.66668301820755 BPR_loss 0.6479194164276123 reg_loss 0.01876361109316349
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 118.78908157348633 | 1519.5074036121368 | 445.2840270996094 | [0.03746762 0.05859793 0.07400438] | [0.01784348 0.02228762 0.02510714] | [0.00213881 0.00168681 0.00142685] | [0.04201687 0.06595348 0.08342035] |
|   4   | 118.78908157348633 | 1312.8636293411255 | 445.2840270996094 | [0.05085887 0.07344293 0.09070357] | [0.02168862 0.02645223 0.02960818] | [0.0029367  0.00212955 0.001759  ] | [0.05777291 0.0832598  0.10274329] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6648654937744141 BPR_loss 0.6455950140953064 reg_loss 0.019270488992333412
epoch: 5 s: 256000 batch_loss 0.6633175611495972 BPR_loss 0.6419509649276733 reg_loss 0.02136661671102047
epoch: 5 s: 512000 batch_loss 0.6653226613998413 BPR_loss 0.6439235210418701 reg_loss 0.02139914594590664
epoch: 5 s: 768000 batch_loss 0.6622804403305054 BPR_loss 0.6413775682449341 reg_loss 0.020902855321764946
epoch: 5 s: 1024000 batch_loss 0.6607049107551575 BPR_loss 0.6391013860702515 reg_loss 0.021603506058454514
epoch: 5 s: 1280000 batch_loss 0.6604633927345276 BPR_loss 0.6397669911384583 reg_loss 0.020696407184004784
