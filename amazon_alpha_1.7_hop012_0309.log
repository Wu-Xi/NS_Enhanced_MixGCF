Namespace(K=1, Ks='[20, 40, 60]', alpha=1.7, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=4, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.693147599697113 BPR_loss 0.6931474208831787 reg_loss 1.7095298687763716e-07
epoch: 0 s: 256000 batch_loss 0.6931070685386658 BPR_loss 0.6930849552154541 reg_loss 2.212433173554018e-05
epoch: 0 s: 512000 batch_loss 0.6920775175094604 BPR_loss 0.6914759874343872 reg_loss 0.0006015123217366636
epoch: 0 s: 768000 batch_loss 0.6888135075569153 BPR_loss 0.6860918998718262 reg_loss 0.002721605123952031
epoch: 0 s: 1024000 batch_loss 0.6859284043312073 BPR_loss 0.6811090111732483 reg_loss 0.004819411784410477
epoch: 0 s: 1280000 batch_loss 0.6830222010612488 BPR_loss 0.6761394143104553 reg_loss 0.006882793735712767
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 185.3307433128357 | 1411.1250262260437 | 459.5483703613281 | [0.02551121 0.04611259 0.06024278] | [0.0114756  0.0157834  0.01838985] | [0.00144511 0.0013164  0.00116288] | [0.02863193 0.05186024 0.06848735] |
|   0   | 185.3307433128357 | 1444.3492832183838 | 459.5483703613281 | [0.04348049 0.06010982 0.07493435] | [0.01779562 0.02130729 0.02401775] | [0.00254239 0.00176301 0.00146829] | [0.05002836 0.06886581 0.08565486] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6768781542778015 BPR_loss 0.6689635515213013 reg_loss 0.007914595305919647
epoch: 1 s: 256000 batch_loss 0.6804805397987366 BPR_loss 0.6718237996101379 reg_loss 0.008656731806695461
epoch: 1 s: 512000 batch_loss 0.6776235699653625 BPR_loss 0.6671739816665649 reg_loss 0.01044959481805563
epoch: 1 s: 768000 batch_loss 0.6751632690429688 BPR_loss 0.6639121770858765 reg_loss 0.011251112446188927
epoch: 1 s: 1024000 batch_loss 0.6730647683143616 BPR_loss 0.6608769297599792 reg_loss 0.01218781154602766
epoch: 1 s: 1280000 batch_loss 0.672766923904419 BPR_loss 0.6591496467590332 reg_loss 0.013617273420095444
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 120.92473292350769 | 1650.7881574630737 | 450.3869323730469 | [0.03142356 0.05251185 0.06717509] | [0.01473543 0.0191725  0.02185331] | [0.00179013 0.00151411 0.0012959 ] | [0.03537186 0.05954945 0.07611815] |
|   1   | 120.92473292350769 | 1412.0621514320374 | 450.3869323730469 | [0.0456306  0.0642447  0.07948885] | [0.0191208  0.02303998 0.02584223] | [0.00264717 0.00187193 0.00155521] | [0.0521004  0.0731911  0.09067345] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.6711539030075073 BPR_loss 0.6576907634735107 reg_loss 0.013463150709867477
epoch: 2 s: 256000 batch_loss 0.6697500348091125 BPR_loss 0.6557611227035522 reg_loss 0.013988900929689407
epoch: 2 s: 512000 batch_loss 0.6691206693649292 BPR_loss 0.6547666788101196 reg_loss 0.014353991486132145
epoch: 2 s: 768000 batch_loss 0.6647224426269531 BPR_loss 0.6503746509552002 reg_loss 0.014347800984978676
epoch: 2 s: 1024000 batch_loss 0.6638457775115967 BPR_loss 0.6475194096565247 reg_loss 0.016326382756233215
epoch: 2 s: 1280000 batch_loss 0.6608358025550842 BPR_loss 0.644122838973999 reg_loss 0.016712959855794907
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 119.96038269996643 | 1614.7211859226227 | 444.8157043457031 | [0.03489643 0.05585298 0.07030044] | [0.01640059 0.02080993 0.02345465] | [0.00198912 0.00161087 0.0013587 ] | [0.03930045 0.06320786 0.07957209] |
|   2   | 119.96038269996643 | 1353.9224150180817 | 444.8157043457031 | [0.04789377 0.06850638 0.08469891] | [0.02036809 0.02471182 0.02768342] | [0.0027681  0.0019903  0.00164989] | [0.05447971 0.07784728 0.09629081] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6593959927558899 BPR_loss 0.6412473320960999 reg_loss 0.018148640170693398
epoch: 3 s: 256000 batch_loss 0.6554589867591858 BPR_loss 0.6379085779190063 reg_loss 0.017550410702824593
epoch: 3 s: 512000 batch_loss 0.6593815684318542 BPR_loss 0.6411408185958862 reg_loss 0.018240757286548615
epoch: 3 s: 768000 batch_loss 0.6614935398101807 BPR_loss 0.6440245509147644 reg_loss 0.017468998208642006
epoch: 3 s: 1024000 batch_loss 0.6596463918685913 BPR_loss 0.6414382457733154 reg_loss 0.01820814423263073
epoch: 3 s: 1280000 batch_loss 0.6594476699829102 BPR_loss 0.6412042379379272 reg_loss 0.01824345998466015
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 119.80217123031616 | 1703.6232635974884 | 440.4415588378906 | [0.0369223  0.05790974 0.0727043 ] | [0.01754697 0.02197399 0.024681  ] | [0.00211289 0.00167147 0.00140482] | [0.04164446 0.06542773 0.082252  ] |
|   3   | 119.80217123031616 | 1380.100930929184  | 440.4415588378906 | [0.04958211 0.07144325 0.08802411] | [0.02117681 0.02579184 0.02882727] | [0.00286265 0.00207263 0.00171094] | [0.05633115 0.08111685 0.09990703] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.658446729183197 BPR_loss 0.6395939588546753 reg_loss 0.018852747976779938
epoch: 4 s: 256000 batch_loss 0.6531322002410889 BPR_loss 0.6319904327392578 reg_loss 0.021141761913895607
epoch: 4 s: 512000 batch_loss 0.6524399518966675 BPR_loss 0.633294939994812 reg_loss 0.019145026803016663
epoch: 4 s: 768000 batch_loss 0.6549519896507263 BPR_loss 0.6346653699874878 reg_loss 0.020286597311496735
epoch: 4 s: 1024000 batch_loss 0.6477468013763428 BPR_loss 0.6262836456298828 reg_loss 0.021463127806782722
epoch: 4 s: 1280000 batch_loss 0.6538388729095459 BPR_loss 0.6337051391601562 reg_loss 0.020133735612034798
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 124.24276447296143 | 1623.5901634693146 | 436.6510314941406 | [0.03841426 0.05966245 0.07522263] | [0.01835178 0.02282773 0.02567009] | [0.00218957 0.00171894 0.00144997] | [0.04307569 0.06723867 0.08486619] |
|   4   | 124.24276447296143 | 1409.6768114566803 | 436.6510314941406 | [0.05093713 0.07331698 0.0908583 ] | [0.02180652 0.02653305 0.02975143] | [0.00293631 0.00212049 0.00176268] | [0.0578123  0.08310223 0.10298752] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6508684754371643 BPR_loss 0.6302064657211304 reg_loss 0.020662035793066025
epoch: 5 s: 256000 batch_loss 0.6487411856651306 BPR_loss 0.6259108185768127 reg_loss 0.022830381989479065
epoch: 5 s: 512000 batch_loss 0.6506247520446777 BPR_loss 0.6277669668197632 reg_loss 0.022857801988720894
epoch: 5 s: 768000 batch_loss 0.6471462249755859 BPR_loss 0.6249812245368958 reg_loss 0.022165007889270782
epoch: 5 s: 1024000 batch_loss 0.6444877982139587 BPR_loss 0.6216598749160767 reg_loss 0.02282790094614029
epoch: 5 s: 1280000 batch_loss 0.6438938975334167 BPR_loss 0.6219810247421265 reg_loss 0.021912869065999985
