Namespace(K=1, Ks='[20, 40, 60]', alpha=1.5, batch_size=2048, batch_test_flag=True, context_hops=3, cuda=True, data_path='data/', dataset='amazon', dim=64, edge_dropout=False, edge_dropout_rate=0.1, epoch=1000, gnn='lightgcn', gpu_id=4, l2=0.001, lr=0.001, mess_dropout=False, mess_dropout_rate=0.1, n_negs=16, neg_mix_num=2, ns='mixgcf', out_dir='./weights/yelp2018/', pool='mean', save=False, test_batch_size=2048, test_flag='part')
reading train and test user-item set ...
building the adj mat ...
loading over ...
卷积器不需要第0层embedding
拿这个最难的neg_item去和pos_item在每一个维度上进行softmax自适应权重,引入alpha超参数来控制模型倾向......
start training ...
epoch: 0 s: 0 batch_loss 0.693147599697113 BPR_loss 0.6931474208831787 reg_loss 1.7024373732965614e-07
epoch: 0 s: 256000 batch_loss 0.6930795907974243 BPR_loss 0.6930446624755859 reg_loss 3.494423071970232e-05
epoch: 0 s: 512000 batch_loss 0.6915843486785889 BPR_loss 0.6907216310501099 reg_loss 0.0008627165807411075
epoch: 0 s: 768000 batch_loss 0.6876685619354248 BPR_loss 0.6842896938323975 reg_loss 0.0033788897562772036
epoch: 0 s: 1024000 batch_loss 0.6845663189888 BPR_loss 0.6790732741355896 reg_loss 0.005493037402629852
epoch: 0 s: 1280000 batch_loss 0.6813607215881348 BPR_loss 0.6738289594650269 reg_loss 0.007531748618930578
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   0   | 184.90071606636047 | 1437.4635109901428 | 458.9651794433594 | [0.02579514 0.04644409 0.06075223] | [0.01161889 0.01593579 0.0185717 ] | [0.001463   0.00132718 0.001172  ] | [0.02898974 0.05232027 0.06902771] |
|   0   | 184.90071606636047 | 1416.558475971222  | 458.9651794433594 | [0.04333728 0.05977209 0.07474593] | [0.01772385 0.02119565 0.02393277] | [0.00252978 0.00175178 0.00146317] | [0.04981564 0.0684325  0.08537911] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 1 s: 0 batch_loss 0.6746385097503662 BPR_loss 0.666100263595581 reg_loss 0.008538267575204372
epoch: 1 s: 256000 batch_loss 0.6787233352661133 BPR_loss 0.6695687770843506 reg_loss 0.009154582396149635
epoch: 1 s: 512000 batch_loss 0.6757834553718567 BPR_loss 0.664862871170044 reg_loss 0.010920594446361065
epoch: 1 s: 768000 batch_loss 0.6733217239379883 BPR_loss 0.6616781949996948 reg_loss 0.01164350938051939
epoch: 1 s: 1024000 batch_loss 0.6708807349205017 BPR_loss 0.6583548784255981 reg_loss 0.012525849044322968
epoch: 1 s: 1280000 batch_loss 0.670570433139801 BPR_loss 0.6566395163536072 reg_loss 0.013930906541645527
+-------+------------------+--------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch | training time(s) |   tesing time(s)   |      Loss     |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+------------------+--------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   1   | 125.128671169281 | 1566.8747310638428 | 449.025390625 | [0.03179336 0.05264467 0.06743956] | [0.01492917 0.01931913 0.02202303] | [0.00181241 0.00151959 0.00130113] | [0.0358173  0.05977582 0.07644675] |
|   1   | 125.128671169281 | 1430.6066451072693 | 449.025390625 | [0.04555411 0.06408444 0.07945353] | [0.01905623 0.02295666 0.02578295] | [0.00263969 0.00186661 0.00155442] | [0.05197435 0.07302565 0.09064982] |
+-------+------------------+--------------------+---------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 2 s: 0 batch_loss 0.668876588344574 BPR_loss 0.6551136374473572 reg_loss 0.013762958347797394
epoch: 2 s: 256000 batch_loss 0.667496383190155 BPR_loss 0.6532378792762756 reg_loss 0.014258510433137417
epoch: 2 s: 512000 batch_loss 0.6667774319648743 BPR_loss 0.6521960496902466 reg_loss 0.014581388793885708
epoch: 2 s: 768000 batch_loss 0.6621624231338501 BPR_loss 0.6476190090179443 reg_loss 0.014543402008712292
epoch: 2 s: 1024000 batch_loss 0.6612554788589478 BPR_loss 0.6447446942329407 reg_loss 0.016510792076587677
epoch: 2 s: 1280000 batch_loss 0.6580963134765625 BPR_loss 0.6411833763122559 reg_loss 0.0169129129499197
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   2   | 115.19947910308838 | 1578.258501291275  | 443.1359558105469 | [0.03509073 0.05602684 0.07033672] | [0.01657616 0.02098784 0.02360799] | [0.0020008  0.00161726 0.0013593 ] | [0.03952682 0.06344883 0.07966702] |
|   2   | 115.19947910308838 | 1365.8676302433014 | 443.1359558105469 | [0.04768981 0.06846521 0.08473667] | [0.0203314  0.02471417 0.02769536] | [0.00275392 0.00198951 0.00164949] | [0.05420396 0.07783153 0.09627505] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 3 s: 0 batch_loss 0.6563626527786255 BPR_loss 0.6380035877227783 reg_loss 0.018359052017331123
epoch: 3 s: 256000 batch_loss 0.6522703766822815 BPR_loss 0.6345574259757996 reg_loss 0.017712976783514023
epoch: 3 s: 512000 batch_loss 0.6564064621925354 BPR_loss 0.6380248665809631 reg_loss 0.018381614238023758
epoch: 3 s: 768000 batch_loss 0.6587138175964355 BPR_loss 0.6411149501800537 reg_loss 0.01759885437786579
epoch: 3 s: 1024000 batch_loss 0.6564895510673523 BPR_loss 0.6381305456161499 reg_loss 0.01835903339087963
epoch: 3 s: 1280000 batch_loss 0.6564715504646301 BPR_loss 0.6381218433380127 reg_loss 0.01834971085190773
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s)  |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   3   | 105.80161666870117 | 1662.756926059723  | 438.4357604980469 | [0.0373199  0.05821746 0.07278618] | [0.01778813 0.0221939  0.02486112] | [0.00213553 0.00168042 0.00140689] | [0.04208989 0.06577093 0.08236153] |
|   3   | 105.80161666870117 | 1391.6564073562622 | 438.4357604980469 | [0.04966735 0.07146422 0.08806249] | [0.02121008 0.0258105  0.02885293] | [0.00286304 0.00207145 0.00171252] | [0.05633115 0.0811011  0.10003309] |
+-------+--------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 4 s: 0 batch_loss 0.6552691459655762 BPR_loss 0.6362875699996948 reg_loss 0.01898154988884926
epoch: 4 s: 256000 batch_loss 0.649509072303772 BPR_loss 0.6282894611358643 reg_loss 0.021219633519649506
epoch: 4 s: 512000 batch_loss 0.6487560868263245 BPR_loss 0.6294816136360168 reg_loss 0.019274480640888214
epoch: 4 s: 768000 batch_loss 0.6514306664466858 BPR_loss 0.6310222148895264 reg_loss 0.02040846273303032
epoch: 4 s: 1024000 batch_loss 0.6436560750007629 BPR_loss 0.6221134662628174 reg_loss 0.021542634814977646
epoch: 4 s: 1280000 batch_loss 0.6503305435180664 BPR_loss 0.63008713722229 reg_loss 0.020243404433131218
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
| Epoch |  training time(s) |   tesing time(s)   |        Loss       |               recall               |                ndcg                |             precision              |             hit_ratio              |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
|   4   | 118.6830952167511 | 1597.7470831871033 | 434.3146057128906 | [0.03886025 0.06012774 0.07556736] | [0.01851712 0.02299012 0.02580816] | [0.00221585 0.00173044 0.00145509] | [0.04362335 0.06768411 0.08515097] |
|   4   | 118.6830952167511 | 1415.9162216186523 | 434.3146057128906 | [0.05106981 0.07351036 0.09091392] | [0.02183269 0.02656491 0.02975841] | [0.00293986 0.00212404 0.00176189] | [0.05789109 0.08325192 0.10297964] |
+-------+-------------------+--------------------+-------------------+------------------------------------+------------------------------------+------------------------------------+------------------------------------+
epoch: 5 s: 0 batch_loss 0.6469724178314209 BPR_loss 0.6262005567550659 reg_loss 0.020771846175193787
epoch: 5 s: 256000 batch_loss 0.6446858048439026 BPR_loss 0.6217378377914429 reg_loss 0.022947978228330612
epoch: 5 s: 512000 batch_loss 0.6465147137641907 BPR_loss 0.6235555410385132 reg_loss 0.022959154099225998
epoch: 5 s: 768000 batch_loss 0.6431214213371277 BPR_loss 0.6208677291870117 reg_loss 0.022253679111599922
epoch: 5 s: 1024000 batch_loss 0.640068531036377 BPR_loss 0.6171878576278687 reg_loss 0.022880662232637405
epoch: 5 s: 1280000 batch_loss 0.6394395232200623 BPR_loss 0.6174368262290955 reg_loss 0.022002682089805603
